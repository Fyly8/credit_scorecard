{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f32db7a",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with over two hundred thousand observations and nearly 800 features.  Each observation is independent from the previous. \n",
    "\n",
    "For each observation, it was recorded whether a default was triggered. In case of a default, the loss was measured. This quantity lies between 0 and 100. It has been normalised, considering that the notional of each transaction at inception is 100. For example, a loss of 60 means that only 40 is reimbursed. If the loan did not default, the loss was 0. You are asked to predict the losses for each observation in the test set.\n",
    "\n",
    "Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Note that some variables may be categorical (e.g. f776 and f777).\n",
    "\n",
    "The competition sponsor has worked to remove time-dimensionality from the data. However, the observations are still listed in order from old to new in the training set. In the test set they are in random order.\n",
    "\n",
    "More info: https://www.kaggle.com/competitions/loan-default-prediction/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eda65f",
   "metadata": {},
   "source": [
    "# 0 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2ac402",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pandas: {pd.__version__}, numpy:{np.__version__}, sklearn:{sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f90864",
   "metadata": {},
   "source": [
    "# 1. Data cleaning\n",
    "and missing values replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb58bde",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04aced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "\n",
    "train_df = pd.read_csv(data_path+\"train_v2.csv.zip\", compression=\"zip\")\n",
    "delay_df = pd.read_csv(data_path+\"test_v2.csv.zip\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at data type, we can observe that there are both numeric & categorical data\n",
    "train_df.shape, train_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3867dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 770 features and target 'loss' at train_df, there is no 'loss' at delay_df\n",
    "delay_df.shape, delay_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['loss'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041435d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loss_stat = (train_df['loss'].value_counts(dropna=False)*100/len(train_df)).sort_index()\n",
    "print(train_loss_stat.head().to_string())\n",
    "train_loss_stat.loc[1:].plot(kind='bar', figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48916a58",
   "metadata": {},
   "source": [
    "There is ~91% of zeros at loss, so lets try to build solution in 2 steps:\n",
    "- binary classification zero or not\n",
    "- regression task for those who not zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7556ee",
   "metadata": {},
   "source": [
    "## split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81ba49",
   "metadata": {},
   "source": [
    "It is better to split data at the very begging in order to evaluate than preprocessing & algorithm at out-of-the-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d63ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = train_df[\"id\"]\n",
    "y = (train_df[\"loss\"]>0).astype(int)\n",
    "X = train_df.drop([\"id\", \"loss\"], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO:DO - u need to make an analysis of features, and add prefixes 'num_' (bool, int, float) and 'cat_' (string, object)\n",
    "# then filter cols at lists\n",
    "num_cols = [j for j in X_train.columns if 'num_' in j]\n",
    "cat_cols = [j for j in X_train.columns if 'cat_' in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test - there is no other cols\n",
    "set(X_train.columns)-set(num_cols)-set(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a5954",
   "metadata": {},
   "source": [
    "## handle Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_list(df: pd.DataFrame, na_threshold: float = 0.75) -> list:\n",
    "    \"\"\"Count na share per col, check if share is more thanna_threshold,\n",
    "    then create list of columns which we shoud drop\"\"\"\n",
    "\n",
    "    cols_na_stat = {}\n",
    "    df_cols = df.columns\n",
    "    # TO:DO\n",
    "    \n",
    "    # then print results\n",
    "    print(\n",
    "        f\"% of cols with na rate > {na_threshold}: {sum_na_cols} obs or {proc_na_cols}%\"\n",
    "    )\n",
    "\n",
    "    return list(cols_na_stat[cols_na_stat>na_threshold].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_na_list = get_na_list(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0042f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(drop_na_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [j for j in X_train.columns if 'num_' in j]\n",
    "cat_cols = [j for j in X_train.columns if 'cat_' in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415b9c8",
   "metadata": {},
   "source": [
    "## fill na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09734b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_dict = {}\n",
    "for col in num_cols:\n",
    "    # TO:DO - fill dict mean_dict with mean values, then replace Nan with them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30762c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO:DO - check cols, in order to see whether or not all cat cols are already encoded\n",
    "# if not - apply LabelEncoder from sklearn.preprocessing\n",
    "(X_train[cat_cols].dtypes=='int64').astype(int).sum() == len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abc649",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_popular_value_dict = {}\n",
    "for col in cat_cols:\n",
    "    # TO:DO - fill dict most_popular_value_dict with most popular values, then replace Nan with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5187007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test - no nan\n",
    "X_train.isna().astype(int).sum()[X_train.isna().astype(int).sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c49345",
   "metadata": {},
   "source": [
    "## handle variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_share_list(df: pd.DataFrame, val_share_threshold: float = 0.75) -> list:\n",
    "\n",
    "    cols_val_share_stat = {}\n",
    "    df_cols = X_train.columns\n",
    "    # TO:DO - code calculation of biggest value share per columns, then filter that if it is > than val_share_threshold\n",
    "    print(\n",
    "        f\"% of cols with biggest value share > {val_share_threshold}: {sum_val_share_cols} obs or {proc_val_share_cols}%\"\n",
    "    )\n",
    "    \n",
    "    return list(cols_val_share_stat[cols_val_share_stat>val_share_threshold].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb295712",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_val_share_list = get_val_share_list(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df2ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(drop_val_share_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d73e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [j for j in X_train.columns if 'num_' in j]\n",
    "cat_cols = [j for j in X_train.columns if 'cat_' in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32e1a4",
   "metadata": {},
   "source": [
    "# 2. Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle outliers using the IQR method\n",
    "def handle_outliers(col_array: np.array, perc_lower=10, perc_upper=90):\n",
    "    \n",
    "    q_lower = np.percentile(col_array, perc_lower)\n",
    "    q_upper = np.percentile(col_array, perc_upper)\n",
    "    iqr = q_upper - q_lower\n",
    "    lower_bound = q_lower - 1.5 * iqr\n",
    "    upper_bound = q_upper + 1.5 * iqr\n",
    "    col_array[col_array < lower_bound] = lower_bound\n",
    "    col_array[col_array > upper_bound] = upper_bound\n",
    "    \n",
    "    return col_array, [lower_bound, upper_bound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c454be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_dict = {j:[] for j in num_cols}\n",
    "for col in num_cols:\n",
    "    X_train[col], outliers_dict[col] = handle_outliers(X_train[col].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1f878",
   "metadata": {},
   "source": [
    "# 3. Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO:DO - anything that help you to get depper understanding of data and find some patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e22073",
   "metadata": {},
   "source": [
    "# 4. Encoding categorical variables \n",
    "using onehot and target encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f5109",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [j for j in X_train.columns if 'cat_' in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ee17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debfda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[cat_cols[0]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9799c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_enc_dict = {}\n",
    "for col in cat_cols:\n",
    "    # TO:DO - refactor that code, in order to make it work in your case\n",
    "    mean_enc_dict[col] = X_train[[col]].join(y_train).groupby(col)['loss'].mean().to_dict()\n",
    "    # hint: 999999999999 is value for other categorical values in test data, which haven`t seen before\n",
    "    mean_enc_dict[col][999999999999] = np.array(list(mean_enc_dict[col].values())).mean()\n",
    "    X_train[col] = X_train[col].map(mean_enc_dict[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6589cc",
   "metadata": {},
   "source": [
    "# 5. Feature selection\n",
    "using correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e598174",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = {}\n",
    "for col in num_cols+cat_cols:\n",
    "    corr_dict[col] = X_train[[col]].join(y_train).corr(method='spearman').iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f59708",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(corr_dict).abs().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96619740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_threshold = 0.8\n",
    "corr_df = X_train.corr(method='spearman').abs()\n",
    "corr_stat_len = 1\n",
    "    \n",
    "while corr_stat_len>0:\n",
    "    corr_stat = (corr_df>corr_threshold).sum().sort_values()-1\n",
    "    corr_stat = corr_stat[corr_stat>0]\n",
    "    try:\n",
    "        col_to_drop = corr_stat.index[0]\n",
    "        corr_df.drop(col_to_drop, axis=0, inplace=True)\n",
    "        corr_df.drop(col_to_drop, axis=1, inplace=True)\n",
    "        corr_stat_len = len(corr_stat)\n",
    "    except IndexError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[list(corr_df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [j for j in X_train.columns if 'num_' in j]\n",
    "cat_cols = [j for j in X_train.columns if 'cat_' in j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960aaa13",
   "metadata": {},
   "source": [
    "# 6. Normalisation\n",
    "Numerical columns  using min-max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17967fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_dict = {}\n",
    "for col in num_cols:\n",
    "    scaler = MinMaxScaler()\n",
    "    # TO:DO - code data scaling using scaler and save that scaler for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bd26b",
   "metadata": {},
   "source": [
    "# Preproc test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0eb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[num_cols+cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    X_test[col] = X_test[col].fillna(mean_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    X_test[col] = X_test[col].fillna(most_popular_value_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa344cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle outliers using the IQR method\n",
    "def handle_outliers_test(col_array: np.array, bound_list):\n",
    "    col_array = col_array.copy()\n",
    "    lower_bound, upper_bound = bound_list\n",
    "    col_array[col_array < lower_bound] = lower_bound\n",
    "    col_array[col_array > upper_bound] = upper_bound\n",
    "    \n",
    "    return col_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    X_test[col] = handle_outliers_test(X_test[col], outliers_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf74453",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    new_cat_val_filter = X_test[col].isin(list(mean_enc_dict[col].keys()))\n",
    "    X_test.loc[new_cat_val_filter==False, col] = 999999999999\n",
    "    X_test[col] = X_test[col].map(mean_enc_dict[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    X_test[col] = scaler_dict[col].transform(X_test[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1], X_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44d9f8",
   "metadata": {},
   "source": [
    "# 7. Modeling\n",
    "using logistic regression, decision tree, random forest and LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31adf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77cf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTEN\n",
    "# sm = SMOTEN(sampling_strategy=0.2, random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd175f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(y_res).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e7ad6",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# all parameters not specified are set to their defaults\n",
    "logistic_model = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    penalty='l2'\n",
    ")\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dbb043",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_train = logistic_model.predict_proba(X_train)[:,1]\n",
    "pd.Series(y_proba_train).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2df69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_train, y_proba_train)\n",
    "print(f\"ROC AUC: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, np.where(y_proba_train>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sklearn_roc_curve(y_real, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "    roc_display.figure_.set_size_inches(5,5)\n",
    "    plt.plot([0, 1], [0, 1], color = 'g')\n",
    "# Plots the ROC curve using the sklearn methods - Good plot\n",
    "plot_sklearn_roc_curve(y_train, y_proba_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test = logistic_model.predict_proba(X_test)[:,1]\n",
    "pd.Series(y_proba_test).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503e5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, np.where(y_proba_test>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09769a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_test, y_proba_test)\n",
    "print(f\"ROC AUC: {score:.4f}\")\n",
    "plot_sklearn_roc_curve(y_test, y_proba_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ac611",
   "metadata": {},
   "source": [
    "## DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = DecisionTreeClassifier(\n",
    "    max_depth = 3,\n",
    "    min_samples_leaf = 100,\n",
    "    random_state = 13\n",
    ")\n",
    "decision_tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(decision_tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb61c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtreeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615973c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_model = dtreeviz.model(decision_tree_model,\n",
    "                           X_train=X_train, y_train=y_train,\n",
    "                           feature_names=X_train.columns,\n",
    "                           target_name='gb',\n",
    "                           class_names=list(y_train.unique()))\n",
    "\n",
    "# v = viz_model.view()     # render as SVG into internal object \n",
    "# v.save(\"/tmp/iris.svg\")  # optionally save as svg\n",
    "viz_model.view()       # in notebook, displays inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DOT data\n",
    "from sklearn.tree import export_graphviz\n",
    "from pydotplus import graph_from_dot_data\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = export_graphviz(decision_tree_model, out_file=None, \n",
    "                           feature_names=X_train.columns,  \n",
    "                           class_names=np.unique(y_train).astype('str'), \n",
    "                           filled=True, rounded=True, special_characters=True)\n",
    "# Draw graph\n",
    "graph = graph_from_dot_data(dot_data)\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_train = decision_tree_model.predict_proba(X_train)[:,1]\n",
    "pd.Series(y_proba_train).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b59277",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_train, y_proba_train)\n",
    "print(f\"ROC AUC: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8173cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, np.where(y_proba_train>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sklearn_roc_curve(y_train, y_proba_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be410245",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test = decision_tree_model.predict_proba(X_test)[:,1]\n",
    "pd.Series(y_proba_test).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, np.where(y_proba_test>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c555d5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_test, y_proba_test)\n",
    "print(f\"ROC AUC: {score:.4f}\")\n",
    "plot_sklearn_roc_curve(y_test, y_proba_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5a859",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40205b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(\n",
    "    max_depth = 3,\n",
    "    min_samples_leaf = 100,\n",
    "    random_state = 13\n",
    ")\n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_train = random_forest_model.predict_proba(X_train)[:,1]\n",
    "pd.Series(y_proba_train).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_train, y_proba_train)\n",
    "print(f\"ROC AUC: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb72bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, np.where(y_proba_train>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sklearn_roc_curve(y_train, y_proba_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa083e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test = random_forest_model.predict_proba(X_test)[:,1]\n",
    "pd.Series(y_proba_test).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf968a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, np.where(y_proba_test>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df8f20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_test, y_proba_test)\n",
    "print(f\"ROC AUC: {score:.4f}\")\n",
    "plot_sklearn_roc_curve(y_test, y_proba_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a35043e",
   "metadata": {},
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96307340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36756196",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model = LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    n_estimators = 100,\n",
    "    max_depth = 3,\n",
    "    learning_rate = 0.02,\n",
    "    colsample_bytree = 0.3,\n",
    "    min_child_samples = 20,\n",
    "    reg_alpha = 2,\n",
    "    objective = 'binary',\n",
    "    is_unbalance = False,\n",
    "    random_state = 21\n",
    ")\n",
    "\n",
    "lightgbm_model.fit(X_train, y_train, eval_metric=['auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad78f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_train = lightgbm_model.predict_proba(X_train)[:,1]\n",
    "pd.Series(y_proba_train).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d09837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_train, y_proba_train)\n",
    "print(f\"ROC AUC: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_train, np.where(y_proba_train>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sklearn_roc_curve(y_train, y_proba_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_test = lightgbm_model.predict_proba(X_test)[:,1]\n",
    "pd.Series(y_proba_test).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944499da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, np.where(y_proba_test>0.5, 1, 0))\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(round(score,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01b1fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = roc_auc_score(y_test, y_proba_test)\n",
    "print(f\"ROC AUC: {score:.4f}\")\n",
    "plot_sklearn_roc_curve(y_test, y_proba_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93e6b3",
   "metadata": {},
   "source": [
    "# 8. Interpretation of results\n",
    "using f1, precision, recal, ROC-AUC and confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dd5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f45d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50e9835e",
   "metadata": {},
   "source": [
    "# 9. Delay prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc81e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_delay = delay_df[num_cols+cat_cols]\n",
    "\n",
    "for col in num_cols:\n",
    "    X_delay[col] = X_delay[col].fillna(mean_dict[col])\n",
    "\n",
    "for col in cat_cols:\n",
    "    X_delay[col] = X_delay[col].fillna(most_popular_value_dict[col])\n",
    "\n",
    "# Define a function to handle outliers using the IQR method\n",
    "def handle_outliers_test(col_array: np.array, bound_list):\n",
    "    col_array = col_array.copy()\n",
    "    lower_bound, upper_bound = bound_list\n",
    "    col_array[col_array < lower_bound] = lower_bound\n",
    "    col_array[col_array > upper_bound] = upper_bound\n",
    "    \n",
    "    return col_array\n",
    "\n",
    "for col in num_cols:\n",
    "    X_delay[col] = handle_outliers_test(X_delay[col], outliers_dict[col])\n",
    "\n",
    "for col in cat_cols:\n",
    "    new_cat_val_filter = X_delay[col].isin(list(mean_enc_dict[col].keys()))\n",
    "    X_delay.loc[new_cat_val_filter==False, col] = 999999999999\n",
    "    X_delay[col] = X_delay[col].map(mean_enc_dict[col])\n",
    "\n",
    "for col in num_cols:\n",
    "    X_delay[col] = scaler_dict[col].transform(X_delay[[col]])\n",
    "\n",
    "X_train.shape[1], X_delay.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae823db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_model.fit(pd.concat([X_train, X_test]), pd.concat([y_train, y_test]), eval_metric=['auc'])\n",
    "y_proba_delay = lightgbm_model.predict_proba(X_delay)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_proba_delay).describe().apply(lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa857ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'proba': y_proba_delay}).to_csv('proba_delay.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
